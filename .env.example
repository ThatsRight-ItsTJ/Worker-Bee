# This library uses chat_init_model from langchain_core.chat_models.base. It supports different models from different providers.
# Check https://python.langchain.com/api_reference/langchain/chat_models/langchain.chat_models.base.init_chat_model.html for more information.
# Here you should set the model, the provider and and all keys that are necessary for the provider.

# OPTION 1: Pollinations AI (Recommended - FREE and unlimited!)
MODEL=openai
MODEL_PROVIDER=pollinations
POLLINATIONS_REFERRER=worker-bee-bolt-v1
# POLLINATIONS_API_KEY=your_key_here  # Optional - for higher rate limits

# OPTION 2: Azure OpenAI (if you have access)
# MODEL=gpt-4o
# MODEL_PROVIDER=azure
# AZURE_OPENAI_API_KEY=
# AZURE_OPENAI_ENDPOINT=

# OPTION 3: OpenAI (if you have OpenAI API key)
# MODEL=gpt-4o
# MODEL_PROVIDER=openai
# OPENAI_API_KEY=

# OPTION 4: Anthropic Claude (if you have Anthropic API key)
# MODEL=claude-3-5-sonnet-20241022
# MODEL_PROVIDER=anthropic
# ANTHROPIC_API_KEY=

# OPTION 5: Groq (Fast and often has free tier)
# MODEL=llama-3.1-70b-versatile
# MODEL_PROVIDER=groq
# GROQ_API_KEY=your_groq_api_key_here

# OPTION 6: Free/Local options
# MODEL=llama3.1:8b
# MODEL_PROVIDER=ollama
# (Requires Ollama to be installed and running locally)

# Optional: LangChain tracing for monitoring and debugging
LANGCHAIN_TRACING_V2=true
LANGCHAIN_ENDPOINT="https://api.smith.langchain.com"
LANGCHAIN_API_KEY=
LANGCHAIN_PROJECT=worker-bee

# LogLevel: Set to debug to enable verbose logging, set to result to get results only. Available: result | debug | info
BROWSER_USE_LOGGING_LEVEL=info

